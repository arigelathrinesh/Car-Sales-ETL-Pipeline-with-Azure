
# ğŸš— Car Sales Data Engineering Project using Azure Data Factory & Delta Lake

This project demonstrates the implementation of a modern data pipeline to ingest, transform, and manage car sales data using **Azure Data Factory (ADF)** and **Azure Data Lake Storage Gen2**. It includes both initial and incremental data loading strategies and follows the **Bronze â†’ Silver â†’ Gold** architecture model using **Delta** format.

---

## ğŸ“ Folder Structure

```
â”œâ”€â”€ data/             # Input CSV files (initial and incremental)
â”‚   â”œâ”€â”€ car_sales.csv
â”‚   â””â”€â”€ incremental_sales.csv
â”‚
â”œâ”€â”€ pipelines/        # ADF pipeline exports (diagnostic.json, info.txt)
â”‚   â”œâ”€â”€ copy_git_pipeline/
â”‚   â””â”€â”€ incremental_pipeline/
â”‚
â”œâ”€â”€ scripts/          # Transformation scripts
â”‚   â”œâ”€â”€ pyspark_notebooks/
â”‚   â””â”€â”€ sql_queries/
â”‚
â”œâ”€â”€ images/           # Screenshots of pipeline runs and clusters
â”‚   â”œâ”€â”€ Copy_Pipeline.png
â”‚   â”œâ”€â”€ Incremental_Pipeline.png
â”‚   â”œâ”€â”€ Job_Run.png
â”‚   â”œâ”€â”€ Job_Run_Details.png
â”‚   â”œâ”€â”€ Cluster_Configuration.png
â”‚   â”œâ”€â”€ Watermark_Setup.png
â”‚   â””â”€â”€ Monitoring_View.png
â”‚
â””â”€â”€ README.md         # Project overview and documentation
```

---

## ğŸš€ Project Workflow (Detailed)

### 1. ğŸ”½ Source Data Acquisition
- Raw car sales data (initial & incremental) is stored on GitHub.
- The pipeline fetches CSV files using Azure Data Factoryâ€™s HTTP dataset connection.
- Ensures consistency by periodically checking GitHub for new records.


---

### 2. ğŸª¨ Bronze Layer Ingestion
- Raw data is ingested using the `copy_git_pipeline`.
- Data is landed as-is into the Bronze zone of the data lake.
- Metadata such as ingestion timestamp is appended.


---

### 3. ğŸ” Incremental Loading Setup
- `Incremental_Pipeline` uses a `last_load` watermark logic.
- Only new records after the previous run timestamp are fetched.
- This helps avoid data duplication and ensures efficient ingestion.


---

### 4. ğŸ§¹ Data Cleansing & Validation
- PySpark notebooks perform:
  - Null value checks and handling.
  - Type casting and formatting.
  - Filtering invalid or duplicate records.
- Results are written to Silver layer as refined Parquet files.


---

### 5. ğŸ—ï¸ Silver Layer Transformation
- Data is enriched and unnecessary columns are dropped.
- Intermediate tables are created for further modeling.
- Joins and aggregations are handled to derive meaningful features.

---

### 6. â­ Gold Layer Fact & Dimension Tables
- Final business-ready tables (`fact_sales`, `dim_model`) created.
- Structured as per star schema for reporting and analysis.
- Stored in Gold zone in partitioned Parquet format.

---

### 7. ğŸ› ï¸ Pipeline Automation & Monitoring
- Pipelines are scheduled using ADF triggers.
- Job logs, errors, and success metrics are monitored.
- Alerts can be configured for failures and row-count mismatches.


---

## ğŸ§° Tools & Technologies Used

- **Azure Data Factory**
- **Azure Data Lake Gen2**
- **Delta Lake / Parquet**
- **PySpark (Databricks Notebooks)**
- **SQL**
- **GitHub**

---

## ğŸ“Š Outcome

This project enables automated, scalable data ingestion and transformation workflows for car sales data, suitable for business analytics and reporting. It demonstrates best practices in designing Azure-native pipelines and data lakehouse solutions.

---

## ğŸ“Œ Author

**Arigela Thrinesh**  
Data Engineering Enthusiast | Azure | PySpark | SQL
